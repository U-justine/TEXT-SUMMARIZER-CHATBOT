import streamlit as st
import PyPDF2
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForQuestionAnswering
import io
import re
import torch
from typing import Optional, List
import logging
import base64
from pathlib import Path

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Set page config
st.set_page_config(
    page_title="‚ö° FastSum PDF AI Assistant | Optimized for Speed",
    page_icon="‚ö°",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# Enhanced Custom CSS for attractive design
st.markdown("""
<style>
    /* Import Google Fonts */
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap');

    /* Hide Streamlit branding */
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    .stDeployButton {visibility: hidden;}

    /* Main container styling */
    .main {
        padding-top: 0rem;
        padding-bottom: 0rem;
    }

    /* Custom hero section */
    .hero-section {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 4rem 2rem;
        margin: -1rem -1rem 3rem -1rem;
        border-radius: 0 0 30px 30px;
        color: white;
        text-align: center;
        position: relative;
        overflow: hidden;
    }

    .hero-section::before {
        content: '';
        position: absolute;
        top: 0;
        left: 0;
        right: 0;
        bottom: 0;
        background: url("data:image/svg+xml,%3Csvg width='60' height='60' viewBox='0 0 60 60' xmlns='http://www.w3.org/2000/svg'%3E%3Cg fill='none' fill-rule='evenodd'%3E%3Cg fill='%23ffffff' fill-opacity='0.1'%3E%3Ccircle cx='30' cy='30' r='2'/%3E%3C/g%3E%3C/g%3E%3C/svg%3E") repeat;
        opacity: 0.3;
    }

    .hero-content {
        position: relative;
        z-index: 1;
    }

    .hero-title {
        font-family: 'Inter', sans-serif;
        font-size: 3.5rem;
        font-weight: 700;
        margin-bottom: 1rem;
        text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
        animation: fadeInUp 1s ease-out;
    }

    .hero-subtitle {
        font-family: 'Inter', sans-serif;
        font-size: 1.3rem;
        font-weight: 300;
        margin-bottom: 2rem;
        opacity: 0.95;
        animation: fadeInUp 1s ease-out 0.2s both;
    }

    .hero-features {
        display: flex;
        justify-content: center;
        gap: 2rem;
        flex-wrap: wrap;
        margin-top: 2rem;
        animation: fadeInUp 1s ease-out 0.4s both;
    }

    .hero-feature {
        background: rgba(255,255,255,0.15);
        backdrop-filter: blur(10px);
        padding: 1rem 1.5rem;
        border-radius: 25px;
        border: 1px solid rgba(255,255,255,0.2);
        font-weight: 500;
    }

    /* Feature cards */
    .feature-card {
        background: linear-gradient(145deg, #ffffff 0%, #f8fafc 100%);
        border-radius: 20px;
        padding: 2rem;
        margin: 1rem 0;
        box-shadow: 0 10px 25px rgba(0,0,0,0.08);
        border: 1px solid rgba(226,232,240,0.8);
        transition: all 0.3s ease;
        position: relative;
        overflow: hidden;
    }

    .feature-card::before {
        content: '';
        position: absolute;
        top: 0;
        left: -100%;
        width: 100%;
        height: 100%;
        background: linear-gradient(90deg, transparent, rgba(255,255,255,0.4), transparent);
        transition: left 0.5s;
    }

    .feature-card:hover::before {
        left: 100%;
    }

    .feature-card:hover {
        transform: translateY(-5px);
        box-shadow: 0 20px 40px rgba(0,0,0,0.12);
    }

    .feature-icon {
        font-size: 3rem;
        margin-bottom: 1rem;
        display: block;
    }

    .feature-title {
        font-family: 'Inter', sans-serif;
        font-size: 1.5rem;
        font-weight: 600;
        color: #1e293b;
        margin-bottom: 0.5rem;
    }

    .feature-desc {
        font-family: 'Inter', sans-serif;
        color: #64748b;
        line-height: 1.6;
        font-size: 1rem;
    }

    /* Upload section */
    .upload-section {
        background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
        border-radius: 20px;
        padding: 2rem;
        margin: 2rem 0;
        color: white;
        text-align: center;
        position: relative;
        overflow: hidden;
    }

    .upload-section::before {
        content: '';
        position: absolute;
        top: -50%;
        left: -50%;
        width: 200%;
        height: 200%;
        background: radial-gradient(circle, rgba(255,255,255,0.1) 0%, transparent 70%);
        animation: rotate 20s linear infinite;
    }

    .upload-content {
        position: relative;
        z-index: 1;
    }

    /* Button styling */
    .stButton > button {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        border: none;
        border-radius: 25px;
        padding: 0.75rem 2rem;
        font-family: 'Inter', sans-serif;
        font-weight: 600;
        font-size: 1rem;
        transition: all 0.3s ease;
        box-shadow: 0 4px 15px rgba(102,126,234,0.4);
        text-transform: uppercase;
        letter-spacing: 0.5px;
    }

    .stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 8px 25px rgba(102,126,234,0.6);
        background: linear-gradient(135deg, #5a6fd8 0%, #6a4190 100%);
    }

    .stButton > button:active {
        transform: translateY(0);
    }

    /* File uploader styling */
    .uploadedFile {
        background: linear-gradient(135deg, #84fab0 0%, #8fd3f4 100%);
        border-radius: 15px;
        padding: 1.5rem;
        margin: 1rem 0;
        border: none;
    }

    /* Progress and status */
    .status-card {
        background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%);
        border-radius: 15px;
        padding: 1.5rem;
        margin: 1rem 0;
        border-left: 5px solid #10b981;
        box-shadow: 0 4px 12px rgba(16,185,129,0.2);
    }

    .error-card {
        background: linear-gradient(135deg, #ffecd2 0%, #fcb69f 100%);
        border-radius: 15px;
        padding: 1.5rem;
        margin: 1rem 0;
        border-left: 5px solid #ef4444;
        color: #7c2d12;
    }

    .warning-card {
        background: linear-gradient(135deg, #fff9c4 0%, #f7dd94 100%);
        border-radius: 15px;
        padding: 1.5rem;
        margin: 1rem 0;
        border-left: 5px solid #f59e0b;
        color: #78350f;
    }

    /* Summary box */
    .summary-box {
        background: linear-gradient(145deg, #ffffff 0%, #f1f5f9 100%);
        border-radius: 20px;
        padding: 2rem;
        margin: 2rem 0;
        box-shadow: 0 10px 30px rgba(0,0,0,0.1);
        border: 1px solid #e2e8f0;
        position: relative;
    }

    .summary-box::before {
        content: '‚ú®';
        position: absolute;
        top: -15px;
        left: 20px;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        width: 40px;
        height: 40px;
        border-radius: 50%;
        display: flex;
        align-items: center;
        justify-content: center;
        font-size: 1.2rem;
    }

    /* Q&A section */
    .qa-response {
        background: linear-gradient(145deg, #f0f9ff 0%, #e0f2fe 100%);
        border-radius: 20px;
        padding: 2rem;
        margin: 1.5rem 0;
        border-left: 5px solid #0ea5e9;
        box-shadow: 0 8px 20px rgba(14,165,233,0.15);
        position: relative;
        overflow: hidden;
    }

    .qa-response::before {
        content: '';
        position: absolute;
        top: 0;
        right: 0;
        width: 100px;
        height: 100px;
        background: radial-gradient(circle, rgba(14,165,233,0.1) 0%, transparent 70%);
    }

    /* Metrics styling */
    .metric-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        border-radius: 15px;
        padding: 1.5rem;
        text-align: center;
        box-shadow: 0 6px 20px rgba(102,126,234,0.3);
    }

    .metric-value {
        font-size: 2rem;
        font-weight: 700;
        margin-bottom: 0.5rem;
        font-family: 'JetBrains Mono', monospace;
    }

    .metric-label {
        font-size: 0.9rem;
        opacity: 0.9;
        text-transform: uppercase;
        letter-spacing: 1px;
    }

    /* Footer */
    .footer {
        background: linear-gradient(135deg, #2d3748 0%, #4a5568 100%);
        color: white;
        padding: 3rem 2rem;
        margin: 4rem -1rem 0 -1rem;
        border-radius: 30px 30px 0 0;
        text-align: center;
    }

    /* Animations */
    @keyframes fadeInUp {
        from {
            opacity: 0;
            transform: translateY(30px);
        }
        to {
            opacity: 1;
            transform: translateY(0);
        }
    }

    @keyframes rotate {
        from { transform: rotate(0deg); }
        to { transform: rotate(360deg); }
    }

    /* Responsive design */
    @media (max-width: 768px) {
        .hero-title { font-size: 2.5rem; }
        .hero-subtitle { font-size: 1.1rem; }
        .hero-features { flex-direction: column; align-items: center; }
        .feature-card { margin: 0.5rem 0; }
    }

    /* Tab styling */
    .stTabs [data-baseweb="tab-list"] {
        gap: 2px;
        background-color: #f1f5f9;
        border-radius: 20px;
        padding: 0.5rem;
    }

    .stTabs [data-baseweb="tab"] {
        height: 60px;
        background-color: transparent;
        border-radius: 15px;
        color: #64748b;
        font-weight: 600;
        font-size: 1rem;
        transition: all 0.3s ease;
    }

    .stTabs [aria-selected="true"] {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        box-shadow: 0 4px 12px rgba(102,126,234,0.4);
    }
</style>
""", unsafe_allow_html=True)

# Cache model loading functions
@st.cache_resource
def load_summarization_model():
    """Load and cache the summarization model - optimized for speed"""
    try:
        # Try multiple models in order of preference
        model_options = [
            "sshleifer/distilbart-cnn-12-6",  # Fast DistilBART
            "facebook/bart-large-cnn",        # Full BART (slower but more accurate)
            "t5-small",                       # Lightweight T5
            "google/pegasus-xsum"             # Alternative option
        ]

        for model_name in model_options:
            try:
                logger.info(f"Attempting to load summarization model: {model_name}")

                # Force CPU usage to avoid CUDA issues
                summarizer = pipeline(
                    "summarization",
                    model=model_name,
                    device=-1,  # Force CPU
                    framework="pt",
                    trust_remote_code=True
                )

                # Test the model with a simple example
                test_text = "This is a test sentence to verify the model works correctly."
                test_result = summarizer(test_text, max_length=20, min_length=5, do_sample=False)

                logger.info(f"Successfully loaded and tested model: {model_name}")
                return summarizer

            except Exception as e:
                logger.warning(f"Failed to load model {model_name}: {e}")
                continue

        # If all models fail
        logger.error("All summarization models failed to load")
        return None

    except Exception as e:
        logger.error(f"Unexpected error in load_summarization_model: {e}")
        return None

@st.cache_resource
def load_qa_model():
    """Load and cache the Q&A model - optimized for speed"""
    try:
        # Try multiple Q&A models in order of preference
        model_options = [
            "distilbert-base-cased-distilled-squad",
            "distilbert-base-uncased-distilled-squad",
            "deepset/roberta-base-squad2",
            "bert-large-uncased-whole-word-masking-finetuned-squad"
        ]

        for model_name in model_options:
            try:
                logger.info(f"Attempting to load Q&A model: {model_name}")

                # Force CPU usage to avoid CUDA issues
                qa_pipeline = pipeline(
                    "question-answering",
                    model=model_name,
                    device=-1,  # Force CPU
                    trust_remote_code=True
                )

                # Test the model with a simple example
                test_context = "This is a test context for the question answering model."
                test_question = "What is this?"
                test_result = qa_pipeline(question=test_question, context=test_context)

                logger.info(f"Successfully loaded and tested Q&A model: {model_name}")
                return qa_pipeline

            except Exception as e:
                logger.warning(f"Failed to load Q&A model {model_name}: {e}")
                continue

        # If all models fail
        logger.error("All Q&A models failed to load")
        return None

    except Exception as e:
        logger.error(f"Unexpected error in load_qa_model: {e}")
        return None

def extract_text_from_pdf(pdf_file) -> str:
    """Extract text from uploaded PDF file - optimized for speed"""
    try:
        import time
        start_time = time.time()

        pdf_reader = PyPDF2.PdfReader(pdf_file)
        text = ""

        # Speed optimization: limit to first 10 pages for faster processing
        max_pages = min(10, len(pdf_reader.pages))

        for page_num in range(max_pages):
            page = pdf_reader.pages[page_num]
            text += page.extract_text() + "\n"

        # Quick text cleaning - simplified for speed
        text = re.sub(r'\s+', ' ', text).strip()

        # Speed optimization: limit text length
        if len(text) > 8000:
            text = text[:8000] + "..."

        end_time = time.time()
        processing_time = end_time - start_time

        if max_pages < len(pdf_reader.pages):
            st.info(f"‚ö° Processed first {max_pages} pages in {processing_time:.1f}s for faster performance")
        else:
            st.success(f"‚ö° Text extracted in {processing_time:.1f} seconds")

        return text
    except Exception as e:
        st.error(f"Error extracting text from PDF: {e}")
        return ""

def chunk_text(text: str, max_length: int = 1024) -> List[str]:
    """Split text into chunks for processing"""
    words = text.split()
    chunks = []
    current_chunk = []
    current_length = 0

    for word in words:
        word_length = len(word) + 1
        if current_length + word_length > max_length:
            if current_chunk:
                chunks.append(' '.join(current_chunk))
                current_chunk = []
                current_length = 0
        current_chunk.append(word)
        current_length += word_length

    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks

def summarize_text_with_options(text: str, summarizer, max_length: int = 130, min_length: int = 30) -> str:
    """Generate summary of the text with custom length options"""
    try:
        import time
        start_time = time.time()

        if not summarizer:
            st.error("üîÑ Model loading failed. Attempting to reload...")
            with st.spinner("Loading summarization model..."):
                summarizer = load_summarization_model()
                if not summarizer:
                    return "‚ùå Summarization model could not be loaded. Please check your internet connection and try refreshing the page."

        # Check text length
        if len(text.strip()) < 50:
            return "‚ö†Ô∏è Text is too short for meaningful summarization. Please provide at least 50 characters."

        # Ensure reasonable limits
        max_length = min(max_length, 512)  # Prevent excessive length
        min_length = max(min_length, 10)   # Ensure minimum reasonable length

        # Split text into chunks if too long
        max_chunk_length = 1024  # Tokens limit for most models
        chunks = chunk_text(text, max_chunk_length)

        if len(chunks) == 1:
            # Single chunk - direct summarization
            try:
                summary_result = summarizer(
                    text,
                    max_length=max_length,
                    min_length=min(min_length, len(text.split())//4),  # Don't exceed 1/4 of original
                    do_sample=False,
                    truncation=True,
                    clean_up_tokenization_spaces=True
                )
                summary = summary_result[0]['summary_text']
            except Exception as e:
                logger.error(f"Error in summarization: {e}")
                # Try with more conservative settings
                try:
                    summary_result = summarizer(
                        text[:2000],  # Limit text length
                        max_length=100,
                        min_length=20,
                        do_sample=False,
                        truncation=True
                    )
                    summary = summary_result[0]['summary_text']
                except:
                    return f"‚ùå Error generating summary. Try with shorter text or refresh the page."
        else:
            # Multiple chunks - summarize each and combine
            chunk_summaries = []
            for i, chunk in enumerate(chunks):
                try:
                    chunk_max = min(max_length // len(chunks) + 20, 100)
                    chunk_min = max(min_length // len(chunks) + 5, 10)

                    chunk_result = summarizer(
                        chunk,
                        max_length=chunk_max,
                        min_length=chunk_min,
                        do_sample=False,
                        truncation=True
                    )
                    chunk_summaries.append(chunk_result[0]['summary_text'])
                except Exception as e:
                    logger.warning(f"Error summarizing chunk {i+1}: {e}")
                    continue

            if chunk_summaries:
                # Combine summaries
                combined_summary = " ".join(chunk_summaries)

                # If combined summary is still too long, summarize it again
                if len(combined_summary.split()) > max_length:
                    try:
                        final_result = summarizer(
                            combined_summary,
                            max_length=max_length,
                            min_length=min_length,
                            do_sample=False,
                            truncation=True
                        )
                        summary = final_result[0]['summary_text']
                    except:
                        summary = combined_summary[:max_length*6]  # Fallback truncation
                else:
                    summary = combined_summary
            else:
                return "‚ùå Could not generate summary from any text chunks. Try with shorter text."

        end_time = time.time()
        processing_time = end_time - start_time

        logger.info(f"Summary generated in {processing_time:.2f} seconds")
        return summary

    except Exception as e:
        logger.error(f"Unexpected error in text summarization: {e}")
        return f"‚ùå Summarization failed. Please try refreshing the page or using shorter text."

def summarize_text(text: str, summarizer) -> str:
    """Generate summary of the text - optimized for speed"""
    try:
        import time
        start_time = time.time()

        if not summarizer:
            return "‚ùå Summarization model not available. Please refresh the page and try again."

        if len(text.split()) < 20:
            return "Text is too short to summarize effectively."

        # Speed optimization: limit text length
        if len(text) > 8000:
            text = text[:8000] + "..."

        # Speed optimization: use smaller chunks and process fewer
        chunks = chunk_text(text, max_length=512)
        summaries = []

        # Process only first 3 chunks for speed
        for i, chunk in enumerate(chunks[:3]):
            if len(chunk.split()) > 15:
                try:
                    # Speed optimizations: shorter summaries, fewer beams
                    summary = summarizer(
                        chunk,
                        max_length=80,      # Reduced from 150
                        min_length=20,      # Reduced from 30
                        do_sample=False,
                        num_beams=2,        # Reduced from 4
                        early_stopping=True
                    )
                    summaries.append(summary[0]['summary_text'])
                except Exception as e:
                    logger.warning(f"Failed to summarize chunk {i}: {e}")
                    continue

        if not summaries:
            return "Unable to generate summary from the provided text."

        combined_summary = ' '.join(summaries)

        # Speed optimization: skip final summarization for single summaries
        if len(summaries) > 1 and len(combined_summary.split()) > 60:
            try:
                final_summary = summarizer(
                    combined_summary,
                    max_length=120,     # Reduced from 200
                    min_length=30,      # Reduced from 50
                    do_sample=False,
                    num_beams=2,
                    early_stopping=True
                )
                combined_summary = final_summary[0]['summary_text']
            except Exception as e:
                logger.warning(f"Final summarization failed: {e}")

        end_time = time.time()
        processing_time = end_time - start_time

        # Add speed info to summary
        speed_info = f"\n\n‚ö° Generated in {processing_time:.1f} seconds"
        return combined_summary + speed_info

    except Exception as e:
        logger.error(f"Error in summarization: {e}")
        return f"Error generating summary: {str(e)}"

def answer_question(question: str, context: str, qa_model) -> dict:
    """Answer question based on the document context - optimized for speed"""
    try:
        import time
        start_time = time.time()

        if not context or not question:
            return {"answer": "Please provide both a question and document context.", "confidence": 0}

        # Speed optimization: reduce context length for faster processing
        max_context_length = 1500
        if len(context) > max_context_length:
            # Smart truncation - try to keep relevant context
            words = context.split()
            question_words = question.lower().split()

            # Find relevant sentences containing question keywords
            sentences = context.split('.')
            relevant_sentences = []
            other_sentences = []

            for sentence in sentences:
                sentence_lower = sentence.lower()
                if any(word in sentence_lower for word in question_words):
                    relevant_sentences.append(sentence)
                else:
                    other_sentences.append(sentence)

            # Combine relevant and other sentences up to limit
            context = '. '.join(relevant_sentences + other_sentences)[:max_context_length]

        # Speed optimization: add parameters for faster inference
        result = qa_model(
            question=question,
            context=context,
            max_answer_len=100,  # Limit answer length for speed
            max_seq_len=512      # Reduce sequence length
        )

        end_time = time.time()
        processing_time = end_time - start_time

        confidence_threshold = 0.1
        if result['score'] < confidence_threshold:
            return {
                "answer": f"I'm not confident enough to answer this question based on the provided document. (Processed in {processing_time:.1f}s)",
                "confidence": result['score']
            }

        # Add timing info to answer
        answer_with_timing = f"{result['answer']}\n\n‚ö° Answered in {processing_time:.1f} seconds"
        return {"answer": answer_with_timing, "confidence": result['score']}

    except Exception as e:
        logger.error(f"Error in question answering: {e}")
        return {"answer": f"Error processing question: {str(e)}", "confidence": 0}

def display_performance_banner():
    """Display speed optimization banner"""
    st.markdown("""
    <div style="background: linear-gradient(135deg, #10b981 0%, #059669 100%); padding: 1rem; border-radius: 15px; margin-bottom: 2rem; text-align: center; color: white;">
        <h3 style="margin-bottom: 0.5rem;">‚ö° Speed Optimized Version</h3>
        <p style="margin: 0; opacity: 0.9;">Now using DistilBART & DistilBERT for 3x faster processing ‚Ä¢ GPU acceleration enabled</p>
    </div>
    """, unsafe_allow_html=True)

def display_hero_section():
    """Display the hero section"""
    st.markdown("""
    <div class="hero-section">
        <div class="hero-content">
            <div class="hero-title">ü§ñ TEXT SUMMARIZER CHATBOT</div>
            <div class="hero-subtitle">
                Transform your documents and text with AI-powered summarization and intelligent Q&A capabilities
            </div>
            <div class="hero-features">
                <div class="hero-feature">üìÑ Smart PDF Processing</div>
                <div class="hero-feature">üìù Direct Text Input</div>
                <div class="hero-feature">üß† AI Summarization</div>
                <div class="hero-feature">üí¨ Intelligent Q&A</div>
                <div class="hero-feature">‚ö° Lightning Fast</div>
            </div>
        </div>
    </div>
    """, unsafe_allow_html=True)

def display_feature_cards():
    """Display feature cards"""
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.markdown("""
        <div class="feature-card">
            <div class="feature-icon">üì§</div>
            <div class="feature-title">PDF Upload</div>
            <div class="feature-desc">
                Upload PDF documents with intelligent text extraction and processing
            </div>
        </div>
        """, unsafe_allow_html=True)

    with col2:
        st.markdown("""
        <div class="feature-card">
            <div class="feature-icon">üìù</div>
            <div class="feature-title">Text Input</div>
            <div class="feature-desc">
                Paste any text directly - articles, essays, reports, or research papers
            </div>
        </div>
        """, unsafe_allow_html=True)

    with col3:
        st.markdown("""
        <div class="feature-card">
            <div class="feature-icon">üéØ</div>
            <div class="feature-title">AI Summarization</div>
            <div class="feature-desc">
                Get concise, accurate summaries powered by state-of-the-art transformer models
            </div>
        </div>
        """, unsafe_allow_html=True)

    with col4:
        st.markdown("""
        <div class="feature-card">
            <div class="feature-icon">ü§ñ</div>
            <div class="feature-title">Interactive Q&A</div>
            <div class="feature-desc">
                Ask questions about your content and get intelligent, contextual answers
            </div>
        </div>
        """, unsafe_allow_html=True)

def initialize_session_state():
    """Initialize session state variables"""
    if 'extracted_text' not in st.session_state:
        st.session_state.extracted_text = ""
    if 'summary' not in st.session_state:
        st.session_state.summary = ""
    if 'qa_history' not in st.session_state:
        st.session_state.qa_history = []
    if 'file_processed' not in st.session_state:
        st.session_state.file_processed = False
    if 'user_input_text' not in st.session_state:
        st.session_state.user_input_text = ""
    if 'text_summary' not in st.session_state:
        st.session_state.text_summary = ""
    if 'sample_text' not in st.session_state:
        st.session_state.sample_text = ""

def display_metrics(text: str):
    """Display text metrics"""
    if not text:
        return

    words = len(text.split())
    chars = len(text)
    sentences = len([s for s in text.split('.') if s.strip()])

    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-value">{words:,}</div>
            <div class="metric-label">Words</div>
        </div>
        """, unsafe_allow_html=True)

    with col2:
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-value">{chars:,}</div>
            <div class="metric-label">Characters</div>
        </div>
        """, unsafe_allow_html=True)

    with col3:
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-value">{sentences:,}</div>
            <div class="metric-label">Sentences</div>
        </div>
        """, unsafe_allow_html=True)

    with col4:
        reading_time = max(1, words // 200)
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-value">{reading_time}</div>
            <div class="metric-label">Min Read</div>
        </div>
        """, unsafe_allow_html=True)

def display_speed_metrics(processing_times=None):
    """Display real-time performance metrics"""
    if processing_times:
        col1, col2, col3, col4 = st.columns(4)

        with col1:
            st.metric("üìÑ Extraction", f"{processing_times.get('extraction', 0):.1f}s",
                     delta="-60% faster" if processing_times.get('extraction', 0) < 3 else None)
        with col2:
            st.metric("üß† Summarization", f"{processing_times.get('summarization', 0):.1f}s",
                     delta="-70% faster" if processing_times.get('summarization', 0) < 8 else None)
        with col3:
            st.metric("üí¨ Q&A Response", f"{processing_times.get('qa', 0):.1f}s",
                     delta="-50% faster" if processing_times.get('qa', 0) < 5 else None)
        with col4:
            total_time = sum(processing_times.values())
            st.metric("‚ö° Total Time", f"{total_time:.1f}s",
                     delta="üöÄ Optimized" if total_time < 15 else None)

def main():
    """Main application function"""
    initialize_session_state()

    # Display performance banner
    display_performance_banner()

    # Display hero section
    display_hero_section()

    # Display feature cards
    st.markdown("<br>", unsafe_allow_html=True)
    display_feature_cards()

    # Add performance note
    st.info("üí° **Performance Tips:** For fastest results, use PDFs under 5MB with clear text. Processing limited to first 10 pages and 8000 characters for optimal speed.")

    # Main application tabs
    st.markdown("<br><br>", unsafe_allow_html=True)
    tab1, tab2, tab3, tab4 = st.tabs(["üì§ Upload & Process", "üìù Text Input", "‚ö° Fast Summary", "üöÄ Speed Q&A"])

    with tab1:
        st.markdown("""
        <div class="upload-section">
            <div class="upload-content">
                <h2 style="margin-bottom: 1rem;">üìÑ Upload Your PDF Document</h2>
                <p style="font-size: 1.1rem; margin-bottom: 1rem;">
                    Get started by uploading a PDF document to unlock AI-powered insights
                </p>
                <div style="background: linear-gradient(135deg, #10b981, #059669); color: white; padding: 0.75rem 1.5rem; border-radius: 25px; display: inline-block; margin-bottom: 2rem; font-weight: 500;">
                    ‚ö° Now 3x faster with optimized DistilBART & DistilBERT models!
                </div>
            </div>
        </div>
        """, unsafe_allow_html=True)

        uploaded_file = st.file_uploader(
            "Choose a PDF file",
            type="pdf",
            help="Upload a PDF document to extract text and generate summaries",
            label_visibility="collapsed"
        )

        if uploaded_file is not None:
            file_size = uploaded_file.size / (1024 * 1024)  # MB

            st.markdown(f"""
            <div class="status-card">
                <h4>üìÑ File Information</h4>
                <p><strong>Name:</strong> {uploaded_file.name}</p>
                <p><strong>Size:</strong> {file_size:.2f} MB</p>
                <p><strong>Type:</strong> {uploaded_file.type}</p>
            </div>
            """, unsafe_allow_html=True)

            col1, col2, col3 = st.columns([1, 2, 1])
            with col2:
                if st.button("üîç Extract & Process Text", key="extract_btn", use_container_width=True):
                    with st.spinner("üîÑ Extracting text from PDF..."):
                        extracted_text = extract_text_from_pdf(uploaded_file)

                    if extracted_text:
                        st.session_state.extracted_text = extracted_text
                        st.session_state.file_processed = True

                        st.markdown("""
                        <div class="status-card">
                            <h4>‚úÖ Text Extraction Successful!</h4>
                            <p>Your document has been processed and is ready for analysis.</p>
                        </div>
                        """, unsafe_allow_html=True)

                        # Display metrics
                        display_metrics(extracted_text)

                    else:
                        st.markdown("""
                        <div class="error-card">
                            <h4>‚ùå Extraction Failed</h4>
                            <p>Could not extract text from the PDF. Please ensure it's not password-protected or corrupted.</p>
                        </div>
                        """, unsafe_allow_html=True)

        # Display extracted text if available
        if st.session_state.extracted_text:
            st.markdown("### üìñ Extracted Content")
            with st.expander("View extracted text", expanded=False):
                st.text_area(
                    "Document content",
                    st.session_state.extracted_text,
                    height=300,
                    disabled=True,
                    label_visibility="collapsed"
                )

    with tab2:
        st.markdown("### üìù Direct Text Input & Summary")
        st.markdown("""
        <div class="upload-section">
            <div class="upload-content">
                <h2 style="margin-bottom: 1rem;">‚úçÔ∏è Paste Your Text</h2>
                <p style="font-size: 1.1rem; margin-bottom: 1rem;">
                    Directly paste or type your text to get instant AI-powered summaries
                </p>
                <div style="background: linear-gradient(135deg, #f59e0b, #d97706); color: white; padding: 0.75rem 1.5rem; border-radius: 25px; display: inline-block; margin-bottom: 2rem; font-weight: 500;">
                    üìù No PDF required - Just paste and summarize!
                </div>
            </div>
        </div>
        """, unsafe_allow_html=True)

        # Sample text examples
        st.markdown("#### üí° Try These Examples")

        sample_texts = {
            "News Article": """
            Scientists at MIT have developed a new artificial intelligence system that can generate highly realistic images from text descriptions. The breakthrough technology, called DALL-E 3, represents a significant advancement in the field of generative AI. The system can create detailed artwork, photographs, and illustrations based on natural language prompts provided by users. Researchers believe this technology could revolutionize creative industries, from advertising and marketing to art and design. The AI model was trained on millions of image-text pairs, allowing it to understand complex relationships between visual elements and linguistic descriptions. Early tests show that the system can generate images with unprecedented accuracy and creativity, often producing results that are indistinguishable from human-created content. However, experts also warn about potential misuse of such technology, particularly in creating deepfakes or misleading visual content. The research team is working on implementing safety measures and ethical guidelines to ensure responsible deployment of this powerful technology.
            """.strip(),

            "Scientific Abstract": """
            Background: Climate change poses significant challenges to global food security, particularly affecting crop yields in developing regions. This study examines the impact of rising temperatures and changing precipitation patterns on wheat production in South Asia. Methods: We analyzed climate data from 1990 to 2020 across five countries and correlated temperature and rainfall variations with wheat yield statistics. Machine learning models were employed to predict future yield scenarios under different climate change projections. Results: Our analysis reveals a strong negative correlation between temperature increases above 2¬∞C and wheat productivity. Regions experiencing irregular rainfall patterns showed up to 25% reduction in yields compared to historical averages. The predictive models suggest that without adaptation strategies, wheat production could decline by 40% by 2050. Conclusions: Urgent implementation of climate-resilient agricultural practices is essential to maintain food security in the region. These findings highlight the need for international cooperation in developing sustainable farming techniques and climate adaptation technologies.
            """.strip(),

            "Business Report": """
            Executive Summary: The global e-commerce market experienced unprecedented growth during 2023, with total sales reaching $6.2 trillion, representing a 15% increase from the previous year. Mobile commerce accounted for 60% of all online transactions, highlighting the continued shift toward smartphone-based shopping experiences. Key market drivers include improved internet infrastructure, enhanced payment security, and changing consumer behavior patterns accelerated by recent global events. North America and Asia-Pacific regions dominated the market, contributing 70% of total e-commerce revenue. Small and medium enterprises (SMEs) showed remarkable adaptability, with 78% expanding their online presence during the reporting period. Challenges remain in logistics optimization, customer data privacy, and sustainable packaging solutions. Looking ahead, artificial intelligence integration, voice commerce, and augmented reality shopping experiences are expected to drive the next wave of e-commerce innovation. Industry experts project continued growth of 12-14% annually through 2025, contingent upon technological advancement and regulatory stability.
            """.strip()
        }

        col1, col2, col3 = st.columns(3)
        with col1:
            if st.button("üì∞ News Article", use_container_width=True):
                st.session_state.sample_text = sample_texts["News Article"]
        with col2:
            if st.button("üî¨ Scientific Abstract", use_container_width=True):
                st.session_state.sample_text = sample_texts["Scientific Abstract"]
        with col3:
            if st.button("üìä Business Report", use_container_width=True):
                st.session_state.sample_text = sample_texts["Business Report"]

        # Initialize sample_text if not exists
        if 'sample_text' not in st.session_state:
            st.session_state.sample_text = ""

        # Text input area
        user_text = st.text_area(
            "Enter your text here:",
            value=st.session_state.sample_text,
            placeholder="Paste your article, essay, research paper, news story, or any long text that you want to summarize...\n\nTip: Try clicking one of the sample buttons above to get started!",
            height=300,
            max_chars=10000,
            help="You can paste up to 10,000 characters for summarization"
        )

        # Clear sample text after user starts typing
        if user_text != st.session_state.sample_text and st.session_state.sample_text != "":
            st.session_state.sample_text = ""

        if user_text:
            # Display text statistics
            word_count = len(user_text.split())
            char_count = len(user_text)

            st.markdown(f"""
            <div class="status-card">
                <h4>üìä Text Statistics</h4>
                <p><strong>Characters:</strong> {char_count:,}</p>
                <p><strong>Words:</strong> {word_count:,}</p>
                <p><strong>Estimated reading time:</strong> {word_count // 200 + 1} minutes</p>
            </div>
            """, unsafe_allow_html=True)

            # Summary options
            col1, col2 = st.columns(2)
            with col1:
                summary_length = st.selectbox(
                    "Summary length:",
                    ["Short", "Medium", "Long"],
                    help="Choose how detailed you want the summary to be"
                )

            with col2:
                summary_style = st.selectbox(
                    "Summary style:",
                    ["Extractive", "Abstractive"],
                    help="Extractive: Uses sentences from original text. Abstractive: Creates new sentences"
                )

            # Generate summary button
            col1, col2, col3 = st.columns([1, 2, 1])
            with col2:
                if st.button("‚ú® Generate Summary from Text", key="text_summary_btn", use_container_width=True):
                    if len(user_text.strip()) < 50:
                        st.warning("‚ö†Ô∏è Please enter at least 50 characters for meaningful summarization.")
                    else:
                        with st.spinner("ü§ñ AI is analyzing your text..."):
                            summarizer = load_summarization_model()
                            if summarizer:
                                # Set max_length based on user preference
                                if summary_length == "Short":
                                    max_len, min_len = 50, 25
                                elif summary_length == "Medium":
                                    max_len, min_len = 100, 50
                                else:  # Long
                                    max_len, min_len = 200, 100

                                # Store the text and generate summary
                                st.session_state.user_input_text = user_text
                                summary = summarize_text_with_options(user_text, summarizer, max_len, min_len)
                                st.session_state.text_summary = summary
                            else:
                                st.session_state.text_summary = "‚ùå Summarization model failed to load. Please refresh the page and try again."

            # Display summary if generated
            if hasattr(st.session_state, 'text_summary') and st.session_state.text_summary:
                st.markdown(f"""
                <div class="summary-box">
                    <h4 style="margin-top: 2rem; margin-bottom: 1rem; color: #1e293b;">üìù Text Summary</h4>
                    <p style="font-size: 1.1rem; line-height: 1.6; color: #374151;">
                        {st.session_state.text_summary}
                    </p>
                </div>
                """, unsafe_allow_html=True)

                # Summary metrics for user input
                if hasattr(st.session_state, 'user_input_text'):
                    original_words = len(st.session_state.user_input_text.split())
                    summary_words = len(st.session_state.text_summary.split())
                    compression_ratio = (summary_words / original_words) * 100 if original_words > 0 else 0

                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.markdown(f"""
                        <div class="metric-card">
                            <div class="metric-value">{original_words:,}</div>
                            <div class="metric-label">Original Words</div>
                        </div>
                        """, unsafe_allow_html=True)

                    with col2:
                        st.markdown(f"""
                        <div class="metric-card">
                            <div class="metric-value">{summary_words:,}</div>
                            <div class="metric-label">Summary Words</div>
                        </div>
                        """, unsafe_allow_html=True)

                    with col3:
                        st.markdown(f"""
                        <div class="metric-card">
                            <div class="metric-value">{compression_ratio:.1f}%</div>
                            <div class="metric-label">Compression</div>
                        </div>
                        """, unsafe_allow_html=True)

                # Question answering for user input text
                st.markdown("### ü§ñ Ask Questions About Your Text")
                question_about_text = st.text_input(
                    "Ask a question about your text:",
                    placeholder="e.g., What are the main points discussed?",
                    key="text_question_input"
                )

                if question_about_text and st.button("üöÄ Get Answer", key="text_ask_btn"):
                    with st.spinner("‚ö° AI analyzing your question..."):
                        qa_model = load_qa_model()
                        if qa_model:
                            result = answer_question(question_about_text, st.session_state.user_input_text, qa_model)

                            st.markdown(f"""
                            <div class="qa-result">
                                <h4 style="color: #1e293b; margin-bottom: 0.5rem;">‚ùì Question</h4>
                                <p style="font-style: italic; margin-bottom: 1rem;">{question_about_text}</p>

                                <h4 style="color: #1e293b; margin-bottom: 0.5rem;">üí° Answer</h4>
                                <p style="font-size: 1.1rem; line-height: 1.6; margin-bottom: 1rem;">{result["answer"]}</p>
                            </div>
                            """, unsafe_allow_html=True)

    with tab3:
        st.markdown("### ‚ú® AI-Powered Text Summary")

        if not st.session_state.extracted_text:
            st.markdown("""
            <div class="warning-card">
                <h4>üìÑ No Document Loaded</h4>
                <p>Please upload and process a PDF document first to generate a summary.</p>
            </div>
            """, unsafe_allow_html=True)
        else:
            col1, col2, col3 = st.columns([1, 2, 1])
            with col2:
                if st.button("‚ú® Generate AI Summary", key="summary_btn", use_container_width=True):
                    with st.spinner("ü§ñ AI is analyzing your document..."):
                        summarizer = load_summarization_model()
                        if summarizer:
                            summary = summarize_text(st.session_state.extracted_text, summarizer)
                            st.session_state.summary = summary
                        else:
                            st.session_state.summary = "‚ùå Summarization model failed to load. Please refresh the page and try again."

            if st.session_state.summary:
                st.markdown(f"""
                <div class="summary-box">
                    <h4 style="margin-top: 1rem; margin-bottom: 1rem; color: #1e293b;">üìù Document Summary</h4>
                    <p style="font-size: 1.1rem; line-height: 1.6; color: #374151;">
                        {st.session_state.summary}
                    </p>
                </div>
                """, unsafe_allow_html=True)

                # Summary metrics
                if st.session_state.extracted_text:
                    original_words = len(st.session_state.extracted_text.split())
                    summary_words = len(st.session_state.summary.split())
                    compression_ratio = (summary_words / original_words) * 100 if original_words > 0 else 0

                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.markdown(f"""
                        <div class="metric-card">
                            <div class="metric-value">{original_words:,}</div>
                            <div class="metric-label">Original Words</div>
                        </div>
                        """, unsafe_allow_html=True)

                    with col2:
                        st.markdown(f"""
                        <div class="metric-card">
                            <div class="metric-value">{summary_words:,}</div>
                            <div class="metric-label">Summary Words</div>
                        </div>
                        """, unsafe_allow_html=True)

                    with col3:
                        st.markdown(f"""
                        <div class="metric-card">
                            <div class="metric-value">{compression_ratio:.1f}%</div>
                            <div class="metric-label">Compression</div>
                        </div>
                        """, unsafe_allow_html=True)

    with tab4:
        st.markdown("### ü§ñ AI Question & Answer Assistant")

        if not st.session_state.extracted_text:
            st.markdown("""
            <div class="warning-card">
                <h4>üìÑ No Document Context</h4>
                <p>Please upload and process a PDF document first to ask questions about its content.</p>
            </div>
            """, unsafe_allow_html=True)
        else:
            question = st.text_input(
                "Ask a question about your document:",
                placeholder="e.g., What is the main topic of this document?",
                key="question_input"
            )

            col1, col2, col3 = st.columns([1, 2, 1])
            with col2:
                if st.button("üöÄ Get Instant Answer", key="ask_btn", use_container_width=True):
                    if question:
                        with st.spinner("‚ö° AI analyzing at high speed..."):
                            qa_model = load_qa_model()
                            if qa_model:
                                result = answer_question(question, st.session_state.extracted_text, qa_model)
                            else:
                                result = {
                                    "answer": "‚ùå Q&A model failed to load. Please refresh the page and try again.",
                                    "confidence": 0
                                }

                        # Store in history
                        st.session_state.qa_history.append({
                            "question": question,
                            "answer": result["answer"],
                            "confidence": result["confidence"]
                        })

                        # Display answer
                        confidence_color = "#10b981" if result["confidence"] > 0.7 else "#f59e0b" if result["confidence"] > 0.3 else "#ef4444"
                        confidence_text = "High" if result["confidence"] > 0.7 else "Medium" if result["confidence"] > 0.3 else "Low"

                        st.markdown(f"""
                        <div class="qa-response">
                            <h4 style="color: #0ea5e9; margin-bottom: 1rem;">‚ùì Question</h4>
                            <p style="font-style: italic; margin-bottom: 1.5rem; font-size: 1.1rem;">"{question}"</p>

                            <h4 style="color: #0ea5e9; margin-bottom: 1rem;">üí° Answer</h4>
                            <p style="font-size: 1.1rem; line-height: 1.6; margin-bottom: 1rem;">{result["answer"]}</p>

                            <div style="display: flex; align-items: center; gap: 1rem;">
                                <div style="background: {confidence_color}; color: white; padding: 0.3rem 1rem; border-radius: 20px; font-size: 0.9rem; font-weight: 600;">
                                    üéØ {confidence_text} Confidence ({result["confidence"]:.1%})
                                </div>
                            </div>
                        </div>
                        """, unsafe_allow_html=True)

                    else:
                        st.markdown("""
                        <div class="warning-card">
                            <h4>‚ùì No Question Provided</h4>
                            <p>Please enter a question about your document to get started.</p>
                        </div>
                        """, unsafe_allow_html=True)

            # Display Q&A History
            if st.session_state.qa_history:
                st.markdown("### üí¨ Conversation History")

                # Show recent conversations
                for i, qa in enumerate(reversed(st.session_state.qa_history[-5:])):
                    with st.expander(f"üí≠ {qa['question'][:60]}...", expanded=(i == 0)):
                        confidence_color = "#10b981" if qa["confidence"] > 0.7 else "#f59e0b" if qa["confidence"] > 0.3 else "#ef4444"
                        confidence_text = "High" if qa["confidence"] > 0.7 else "Medium" if qa["confidence"] > 0.3 else "Low"

                        st.markdown(f"""
                        <div style="background: #f8fafc; padding: 1rem; border-radius: 10px; margin: 0.5rem 0;">
                            <p><strong>‚ùì Question:</strong> {qa['question']}</p>
                            <p><strong>üí° Answer:</strong> {qa['answer']}</p>
                            <div style="background: {confidence_color}; color: white; padding: 0.3rem 1rem; border-radius: 15px; display: inline-block; font-size: 0.8rem; margin-top: 0.5rem;">
                                {confidence_text} Confidence ({qa['confidence']:.1%})
                            </div>
                        </div>
                        """, unsafe_allow_html=True)

    # Footer
    st.markdown("---")
    st.markdown("### üöÄ PDF AI Assistant")
    st.markdown("*Powered by state-of-the-art AI models from Hugging Face*")

    col1, col2, col3 = st.columns(3)
    with col1:
        st.info("ü§ñ **BART Large CNN**\nText Summarization")
    with col2:
        st.success("üí¨ **RoBERTa Squad2**\nQuestion Answering")
    with col3:
        st.warning("üìÑ **PyPDF2 Engine**\nDocument Processing")

    st.markdown("---")
    st.markdown("Built with ‚ù§Ô∏è using Streamlit ‚Ä¢ Transform your documents with AI")
    st.caption("Upload ‚Ä¢ Extract ‚Ä¢ Summarize ‚Ä¢ Ask Questions")

    # Add some breathing room at the bottom
    st.markdown("<br><br>", unsafe_allow_html=True)

if __name__ == "__main__":
    main()
